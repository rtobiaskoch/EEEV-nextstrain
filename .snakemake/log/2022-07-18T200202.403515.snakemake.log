Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
align            1              1              1
ancestral        1              1              1
clades           1              1              1
export           1              1              1
filter           1              1              1
refine           1              1              1
traits           1              1              1
translate        1              1              1
tree             1              1              1
total            9              1              1

Select jobs to execute...

[Mon Jul 18 20:02:02 2022]
Job 4: 
        Filtering to
          - 100 sequence(s) per country year month
          - from 1900 onwards
          - excluding strains in config/dropped_strains.txt
        

[Mon Jul 18 20:02:03 2022]
Error in rule filter:
    jobid: 4
    output: results/filtered.fasta
    shell:
        
        augur filter             --sequences data/study_dataset.fasta             --metadata data/study_dataset.csv             --exclude config/dropped_strains.txt             --output results/filtered.fasta             --group-by country year month             --sequences-per-group 100             --min-date 1900
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/user/GLab Dropbox/GLab_team/Projects/2022_EEEV/EEEV-nextstrain/.snakemake/log/2022-07-18T200202.403515.snakemake.log
